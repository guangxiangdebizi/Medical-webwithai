"""
MCPæ™ºèƒ½ä½“å°è£… - ä¸ºWebåç«¯ä½¿ç”¨
åŸºäº test.py ä¸­çš„ SimpleMCPAgentï¼Œä¼˜åŒ–ä¸ºé€‚åˆWebSocketæµå¼æ¨é€çš„ç‰ˆæœ¬
"""

import os
import json
import asyncio
from typing import Dict, List, Any, AsyncGenerator, Optional
from pathlib import Path
from datetime import datetime, timedelta

from dotenv import load_dotenv, find_dotenv
import re
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage
import contextvars
import pymysql

# å¯¼å…¥æ¨¡å—åŒ–ç»„ä»¶
from mcp_modules.config import MCPConfig
from mcp_modules.multimodal import MultimodalProcessor
from mcp_modules.model_manager import ModelManager
from mcp_modules.message_processor import MessageProcessor
from get_mcp_tools import MCPToolsManager
from prompt.loader import load_prompts

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1. MCPé…ç½®ç®¡ç† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å·²ç§»è‡³ mcp_agent/config.py


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3. Webç‰ˆMCPæ™ºèƒ½ä½“ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class WebMCPAgent:
    """Webç‰ˆMCPæ™ºèƒ½ä½“ - æ”¯æŒæµå¼æ¨é€"""

    def __init__(self):
        # ä¿®å¤ï¼šä½¿ç”¨backendç›®å½•ä¸‹çš„é…ç½®æ–‡ä»¶
        config_path = Path(__file__).parent / "mcp.json"
        self.config = MCPConfig(str(config_path))
        self.llm = None
        self.llm_tools = None  # ç»‘å®šå·¥å…·ç”¨äºåˆ¤å®šä¸å·¥å…·é˜¶æ®µ
        self.llm_nontool = None  # æ— å·¥å…·å®ä¾‹ï¼Œä»…ç”¨äºå·¥å…·å†…éƒ¨å¦‚SQLé‡å†™
        
        # åˆå§‹åŒ–å·¥å…·ç®¡ç†å™¨
        self.tools_manager = MCPToolsManager()
        # å…¼å®¹æ€§å±æ€§ï¼ˆæŒ‡å‘å·¥å…·ç®¡ç†å™¨çš„å±æ€§ï¼‰
        self.tools = self.tools_manager.tools
        self.tools_by_server = self.tools_manager.tools_by_server
        self.server_configs = self.tools_manager.server_configs
        self.mcp_client = self.tools_manager.mcp_client

        # åŠ è½½ .env å¹¶è®¾ç½®APIç¯å¢ƒå˜é‡ï¼ˆè¦†ç›–å·²å­˜åœ¨çš„ç¯å¢ƒå˜é‡ï¼‰
        try:
            load_dotenv(find_dotenv(), override=True)
        except Exception:
            # å¿½ç•¥ .env åŠ è½½é”™è¯¯ï¼Œç»§ç»­ä»ç³»ç»Ÿç¯å¢ƒè¯»å–
            pass

        # åˆå§‹åŒ–æ¨¡å—åŒ–ç»„ä»¶
        self.model_manager = ModelManager()

        # ä»æ¨¡å‹ç®¡ç†å™¨è·å–é…ç½®ï¼ˆå‘åå…¼å®¹ï¼‰
        self.api_key = self.model_manager.api_key
        self.base_url = self.model_manager.base_url
        self.model_name = self.model_manager.model_name
        self.llm_profiles = self.model_manager.llm_profiles
        self.default_profile_id = self.model_manager.default_profile_id
        self.temperature = self.model_manager.temperature
        self.timeout = self.model_manager.timeout

        # å°†å…³é”®é…ç½®åŒæ­¥åˆ°ç¯å¢ƒï¼ˆä¾›åº•å±‚SDKä½¿ç”¨ï¼‰ï¼Œä¸è¦†ç›–å¤–éƒ¨å·²è®¾å€¼
        if self.api_key and not os.getenv("OPENAI_API_KEY"):
            os.environ["OPENAI_API_KEY"] = self.api_key
        if self.base_url and not os.getenv("OPENAI_BASE_URL"):
            os.environ["OPENAI_BASE_URL"] = self.base_url

        # ä¼šè¯ä¸Šä¸‹æ–‡ï¼ˆå­˜æ”¾æ¯ä¸ª session çš„ msid ç­‰ï¼‰
        self.session_contexts: Dict[str, Dict[str, Any]] = {}
        
        # å†å²å›¾ç‰‡é…ç½®
        try:
            public_base_url = os.getenv("PUBLIC_BASE_URL", "").strip()
            history_image_max_file_bytes = int(os.getenv("HISTORY_IMAGE_MAX_FILE_BYTES", str(2 * 1024 * 1024)))
        except Exception:
            public_base_url = ""
            history_image_max_file_bytes = 2 * 1024 * 1024
            
        self.multimodal_processor = MultimodalProcessor(public_base_url, history_image_max_file_bytes)
        
        try:
            history_images_max_total = int(os.getenv("HISTORY_IMAGES_MAX_TOTAL", "6"))
            history_images_max_per_record = int(os.getenv("HISTORY_IMAGES_MAX_PER_RECORD", "3"))
        except Exception:
            history_images_max_total = 6
            history_images_max_per_record = 3
            
        self.message_processor = MessageProcessor(self.multimodal_processor, history_images_max_total, history_images_max_per_record)

        # æ•°æ®åº“é…ç½®ï¼ˆä»ç¯å¢ƒè¯»å–ï¼Œæä¾›é»˜è®¤å€¼ï¼‰
        self.db_host = os.getenv("DB_HOST", "18.119.46.208")
        self.db_user = os.getenv("DB_USER", "root")
        self.db_password = os.getenv("DB_PASSWORD", "zkshi0101")
        self.db_name = os.getenv("DB_NAME", "ry_vuebak")
        try:
            self.db_port = int(os.getenv("DB_PORT", "3306"))
        except Exception:
            self.db_port = 3306

        # å½“å‰ä¼šè¯IDä¸Šä¸‹æ–‡å˜é‡ï¼ˆç”¨äºå·¥å…·åœ¨è¿è¡Œæ—¶è¯†åˆ«ä¼šè¯ï¼‰
        self._current_session_id_ctx: contextvars.ContextVar = contextvars.ContextVar("current_session_id", default=None)
        
        # è®°å½•ä¸æ”¯æŒå¤šæ¨¡æ€çš„æ¨¡å‹ï¼ˆé¿å…é‡å¤å°è¯•ï¼‰
        self._non_multimodal_models: set = set()

    # å·²ç§»è‡³ mcp_agent/model_manager.py



    # å·²ç§»è‡³ mcp_agent/multimodal.py

    def _get_current_model_key(self, session_id: Optional[str] = None) -> str:
        """è·å–å½“å‰ä¼šè¯ä½¿ç”¨çš„æ¨¡å‹æ ‡è¯†ï¼ˆç”¨äºè®°å½•å¤šæ¨¡æ€æ”¯æŒæƒ…å†µï¼‰"""
        return self.model_manager.get_current_model_key(self.session_contexts, session_id)

    def _convert_multimodal_to_text(self, messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å°†å¤šæ¨¡æ€æ¶ˆæ¯è½¬æ¢ä¸ºçº¯æ–‡æœ¬æ¶ˆæ¯"""
        return self.multimodal_processor.convert_multimodal_to_text(messages)

    def _is_multimodal_error(self, error_str: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå¤šæ¨¡æ€æ ¼å¼ä¸æ”¯æŒçš„é”™è¯¯"""
        return MultimodalProcessor.is_multimodal_error(error_str)


    def get_models_info(self) -> Dict[str, Any]:
        """å¯¹å¤–æš´éœ²çš„æ¨¡å‹æ¡£ä½ä¿¡æ¯ï¼ˆç”¨äºå‰ç«¯å±•ç¤ºï¼‰ã€‚"""
        profiles = self.llm_profiles or {}
        ids = list(profiles.keys())
        non_default_ids = [pid for pid in ids if pid != "default"]

        # è®¡ç®—æœ‰æ•ˆé»˜è®¤æ¡£ä½ï¼šä¼˜å…ˆé‡‡ç”¨ LLM_DEFAULT æŒ‡å®šä¸”å­˜åœ¨çš„IDï¼›
        # å¦åˆ™è‹¥å­˜åœ¨é default æ¡£ä½ï¼Œå–ç¬¬ä¸€ä¸ªï¼›å¦åˆ™åªèƒ½æ˜¯ defaultï¼ˆå•æ¨¡å‹æ—§å…¼å®¹ï¼‰
        if self.default_profile_id and self.default_profile_id != "default" and self.default_profile_id in profiles:
            effective_default = self.default_profile_id
        elif non_default_ids:
            effective_default = non_default_ids[0]
        else:
            effective_default = "default"

        # å±•ç¤ºç­–ç•¥ï¼š
        # - è‹¥å­˜åœ¨ä»»æ„é default æ¡£ä½ï¼Œåˆ™å®Œå…¨éšè— defaultï¼ˆå®ƒåªä½œä¸ºåˆ«å/å›é€€ï¼Œä¸å•ç‹¬æ˜¾ç¤ºï¼‰ã€‚
        # - è‹¥åªæœ‰ default ä¸€ä¸ªæ¡£ä½ï¼Œåˆ™æ˜¾ç¤ºå®ƒï¼ˆæ—§ç‰ˆå•æ¨¡å‹åœºæ™¯ï¼‰ã€‚
        show_ids = non_default_ids if non_default_ids else (["default"] if "default" in profiles else [])

        # ä¸å»é‡ï¼Œå…è®¸åŒä¸€ (base_url, model) çš„å¤šä¸ªæ¡£ä½åŒæ—¶æ˜¾ç¤º
        models = []
        for pid in show_ids:
            cfg = profiles.get(pid, {})
            models.append({
                "id": pid,
                "label": cfg.get("label", pid),
                "model": cfg.get("model", ""),
                "is_default": pid == effective_default,
            })

        # æç«¯å…œåº•ï¼šå¦‚æœæœ€ç»ˆä¸€ä¸ªéƒ½æ²¡æœ‰ï¼ˆç†è®ºä¸ä¼šå‘ç”Ÿï¼‰ï¼Œè¿”å›ç©ºåˆ—è¡¨ä¸é»˜è®¤ID
        return {"models": models, "default": effective_default}

    def _get_or_create_llm_instances(self, profile_id: str) -> Dict[str, Any]:
        """æ ¹æ®æ¡£ä½è·å–/åˆ›å»ºå¯¹åº”çš„ LLM å®ä¾‹é›†åˆï¼šllmã€llm_nontoolã€llm_toolsã€‚"""
        return self.model_manager.get_or_create_llm_instances(profile_id, self.tools)

    async def initialize(self):
        """åˆå§‹åŒ–æ™ºèƒ½ä½“"""
        try:
            # é€‰æ‹©å¯åŠ¨æ¡£ä½ï¼šä¼˜å…ˆ LLM_DEFAULT æŒ‡å®šçš„æ¡£ä½ï¼›å¦åˆ™é€‰æ‹©ä»»ä¸€å« api_key çš„æ¡£ä½ï¼›
            # è‹¥å‡æ— åˆ™å›é€€åˆ°ç¯å¢ƒå˜é‡ OPENAI_API_KEYï¼›ä»æ— åˆ™æŠ¥é”™
            startup_cfg = None
            # ä¼˜å…ˆé»˜è®¤æ¡£ä½
            if self.default_profile_id in self.llm_profiles:
                cfg = self.llm_profiles[self.default_profile_id]
                if cfg.get("api_key") and cfg.get("model"):
                    startup_cfg = cfg
            # å…¶æ¬¡ä»»æ„æœ‰æ•ˆæ¡£ä½
            if startup_cfg is None:
                for _pid, cfg in self.llm_profiles.items():
                    if _pid == "default":
                        continue
                    if cfg.get("api_key") and cfg.get("model"):
                        startup_cfg = cfg
                        break
            # æœ€åå›é€€åˆ°ç¯å¢ƒå˜é‡
            if startup_cfg is None and os.getenv("OPENAI_API_KEY"):
                startup_cfg = {
                    "api_key": os.getenv("OPENAI_API_KEY").strip(),
                    "base_url": os.getenv("OPENAI_BASE_URL", "").strip(),
                    "model": self.model_name,
                    "temperature": self.temperature,
                    "timeout": self.timeout,
                }
            if startup_cfg is None:
                raise RuntimeError("ç¼ºå°‘å¯ç”¨çš„æ¨¡å‹æ¡£ä½æˆ– OPENAI_API_KEYï¼Œè¯·åœ¨ .env ä¸­é…ç½® LLM_PROFILES å¯¹åº”çš„ *_API_KEY æˆ–æä¾› OPENAI_API_KEY")

            # ä¸´æ—¶å†™å…¥ç¯å¢ƒä¾›åº•å±‚ SDK ä½¿ç”¨
            if startup_cfg.get("api_key"):
                os.environ["OPENAI_API_KEY"] = startup_cfg["api_key"]
            if startup_cfg.get("base_url"):
                os.environ["OPENAI_BASE_URL"] = startup_cfg["base_url"]

            # ChatOpenAI æ”¯æŒä»ç¯å¢ƒå˜é‡è¯»å– base_url
            base_llm = ChatOpenAI(
                model=startup_cfg.get("model", self.model_name),
                temperature=startup_cfg.get("temperature", self.temperature),
                timeout=startup_cfg.get("timeout", self.timeout),
                max_retries=3,
            )
            # ä¸»å¼•ç”¨å‘åå…¼å®¹
            self.llm = base_llm
            # æ— å·¥å…·å®ä¾‹ï¼šå½“å‰ä¸ base_llm ç›¸åŒï¼ˆæ— éœ€ç»‘å®šå·¥å…·ï¼‰ï¼Œä¾›å·¥å…·å†…éƒ¨è°ƒç”¨
            self.llm_nontool = ChatOpenAI(
                model=startup_cfg.get("model", self.model_name),
                temperature=startup_cfg.get("temperature", self.temperature),
                timeout=startup_cfg.get("timeout", self.timeout),
                max_retries=3,
            )

            # åŠ è½½MCPé…ç½®å¹¶è¿æ¥
            mcp_config = self.config.load_config()
            server_configs = mcp_config.get("servers", {})
            
            # å‡†å¤‡æ•°æ®åº“é…ç½®
            db_config = {
                'host': self.db_host,
                'user': self.db_user,
                'password': self.db_password,
                'name': self.db_name,
                'port': self.db_port
            }
            
            # ä½¿ç”¨å·¥å…·ç®¡ç†å™¨åˆå§‹åŒ–å·¥å…·
            tools_success = await self.tools_manager.initialize_mcp_tools(
                server_configs=server_configs,
                db_config=db_config,
                session_contexts=self.session_contexts,
                current_session_id_ctx=self._current_session_id_ctx,
                llm_nontool=self.llm_nontool
            )
            
            if not tools_success:
                print("âš ï¸ å·¥å…·åˆå§‹åŒ–å¤±è´¥ï¼Œä½†ç»§ç»­å¯åŠ¨")
            
            # æ›´æ–°å¼•ç”¨ï¼ˆå·¥å…·ç®¡ç†å™¨å¯èƒ½å·²æ›´æ–°è¿™äº›å±æ€§ï¼‰
            self.mcp_client = self.tools_manager.mcp_client

            # åˆ›å»ºå·¥å…·åˆ¤å®šå®ä¾‹ï¼ˆé»˜è®¤æ¡£ä½ï¼‰ï¼Œå…¶ä½™æ¡£ä½åœ¨ç¬¬ä¸€æ¬¡ä½¿ç”¨æ—¶æŒ‰éœ€åˆ›å»º
            self.llm_tools = base_llm.bind_tools(self.tools)

            print("ğŸ¤– Web MCPæ™ºèƒ½åŠ©æ‰‹å·²å¯åŠ¨ï¼")
            return True

        except Exception as e:
            import traceback
            print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
            print(f"ğŸ“‹ è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            traceback.print_exc()
            
            # å°è¯•æ¸…ç†å¯èƒ½çš„è¿æ¥
            if hasattr(self, 'mcp_client') and self.mcp_client:
                try:
                    await self.mcp_client.close()
                except:
                    pass
            return False

    def _get_tools_system_prompt(self, profile_id: Optional[str] = None) -> str:
        """ç”¨äºå·¥å…·åˆ¤å®š/æ‰§è¡Œé˜¶æ®µçš„ç³»ç»Ÿæç¤ºè¯ï¼šæŒ‰æ¡£ä½åŠ è½½æ¨¡æ¿å¹¶æ ¼å¼åŒ–ä¸Šä¸‹æ–‡ã€‚"""
        now = datetime.now()
        current_date = now.strftime("%Yå¹´%mæœˆ%dæ—¥")
        current_weekday = ["å‘¨ä¸€", "å‘¨äºŒ", "å‘¨ä¸‰", "å‘¨å››", "å‘¨äº”", "å‘¨å…­", "å‘¨æ—¥"][now.weekday()]
        prompts = load_prompts(profile_id)
        template = prompts.get("tools_system_prompt_template", "")
        try:
            return template.format(current_date=current_date, current_weekday=current_weekday)
        except Exception:
            return template

    def _get_stream_system_prompt(self) -> str:
        """ä¿æŒæ¥å£ä»¥å…¼å®¹æ—§è°ƒç”¨ï¼Œä½†å½“å‰ä¸å†ä½¿ç”¨æµå¼å›ç­”æç¤ºè¯ã€‚"""
        return ""

    # å·²ç§»è‡³ get_mcp_tools.py

    async def chat_stream(self, user_input, history: List[Dict[str, Any]] = None, session_id: Optional[str] = None) -> AsyncGenerator[Dict[str, Any], None]:
        """æµå¼æ¢æµ‹ + ç«‹å³ä¸­æ–­ï¼š
        - å…ˆç›´æ¥ astream å¼€æµï¼ŒçŸ­æš‚ç¼“å†²å¹¶æ£€æµ‹ function_call/tool_callï¼›
        - è‹¥æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨ï¼šç«‹å³ä¸­æ–­æœ¬æ¬¡æµå¼ï¼ˆä¸ä¸‹å‘ç¼“å†²ï¼‰ï¼Œæ‰§è¡Œå·¥å…·ï¼ˆéæµå¼ï¼‰ï¼Œå†™å› messages åè¿›å…¥ä¸‹ä¸€è½®ï¼›
        - è‹¥æœªæ£€æµ‹åˆ°å·¥å…·ï¼šå°†æœ¬æ¬¡æµä½œä¸ºæœ€ç»ˆå›ç­”ï¼Œå¼€å§‹æµå¼æ¨é€åˆ°ç»“æŸã€‚
        """
        try:
            if session_id:
                try:
                    self._current_session_id_ctx.set(session_id)
                except Exception:
                    pass
            try:
                preview = user_input if isinstance(user_input, str) else "[multimodal parts]"
                if isinstance(preview, str):
                    preview = preview[:50]
                print(f"ğŸ¤– å¼€å§‹å¤„ç†ç”¨æˆ·è¾“å…¥: {preview}...")
            except Exception:
                print("ğŸ¤– å¼€å§‹å¤„ç†ç”¨æˆ·è¾“å…¥ ...")
            yield {"type": "status", "content": "å¼€å§‹ç”Ÿæˆ..."}

            # ä¾æ®ä¼šè¯ä¸Šä¸‹æ–‡é€‰æ‹©æ¨¡å‹æ¡£ä½
            profile_id = None
            try:
                if session_id and self.session_contexts.get(session_id):
                    profile_id = self.session_contexts[session_id].get("model") or self.session_contexts[session_id].get("llm_profile")
            except Exception:
                profile_id = None

            llm_bundle = self._get_or_create_llm_instances(profile_id)
            current_llm_tools = llm_bundle.get("llm_tools", self.llm_tools)

            # 1) æ„å»ºå…±äº«æ¶ˆæ¯å†å²ï¼ˆä¸åŒ…å«ç³»ç»Ÿæç¤ºï¼Œä¾¿äºä¸¤å¥—ç³»ç»Ÿæç¤ºåˆ†åˆ«æ³¨å…¥ï¼‰
            # æ£€æŸ¥å½“å‰æ¨¡å‹æ˜¯å¦å·²çŸ¥ä¸æ”¯æŒå¤šæ¨¡æ€
            current_model_key = self._get_current_model_key(session_id)
            force_text_only = current_model_key in self._non_multimodal_models
            
            shared_history = self.message_processor.build_shared_history(history, user_input, force_text_only)

            max_rounds = 25
            round_index = 0
            # åˆå¹¶ä¸¤é˜¶æ®µè¾“å‡ºä¸ºåŒä¸€æ¡æ¶ˆæ¯ï¼šåœ¨æ•´ä¸ªä¼šè¯å›ç­”æœŸé—´ä»…å‘é€ä¸€æ¬¡ startï¼Œæœ€åä¸€æ¬¡æ€§ end
            combined_response_started = False
            while round_index < max_rounds:
                round_index += 1
                print(f"ğŸ§  ç¬¬ {round_index} è½®æ¨ç† (åŒå®ä¾‹ï¼šåˆ¤å®šå·¥å…· + çº¯æµå¼å›ç­”)...")

                # 2) ä½¿ç”¨å¸¦å·¥å…·å®ä¾‹åš"æµå¼åˆ¤å®š"ï¼š
                tools_messages = [{"role": "system", "content": self._get_tools_system_prompt(profile_id)}] + shared_history
                tool_calls_check = None
                last_usage: Optional[Dict[str, Any]] = None
                buffered_chunks: List[str] = []
                content_preview = ""
                response_started = False
                multimodal_fallback_attempted = False
                
                try:
                    # æŠ‘åˆ¶MCPå®¢æˆ·ç«¯åœ¨åˆ¤å®šå·¥å…·æ—¶çš„SSEè§£æé”™è¯¯æ—¥å¿—
                    import logging
                    mcp_logger = logging.getLogger('mcp')
                    original_level = mcp_logger.level
                    mcp_logger.setLevel(logging.CRITICAL)
                    
                    try:
                        async for event in current_llm_tools.astream_events(tools_messages, version="v1"):
                            ev = event.get("event")
                            if ev == "on_chat_model_stream":
                                data = event.get("data", {})
                                chunk = data.get("chunk")
                                if chunk is None:
                                    continue
                                try:
                                    content_piece = getattr(chunk, 'content', None)
                                except Exception:
                                    content_piece = None
                                if content_piece:
                                    # ç«‹å³å‘å‰ç«¯æµå¼ä¸‹å‘ä½œä¸ºæœ€ç»ˆå›å¤ï¼ˆåˆå¹¶æ¨¡å¼ï¼šä»…é¦–æ¬¡å‘é€ startï¼‰
                                    if not combined_response_started:
                                        yield {"type": "ai_response_start", "content": "AIæ­£åœ¨å›å¤..."}
                                        combined_response_started = True
                                    response_started = True
                                    buffered_chunks.append(content_piece)
                                    try:
                                        print(f"ğŸ“¤ [åˆ¤å®šLLMæµ] {content_piece}")
                                    except Exception:
                                        pass
                                    yield {"type": "ai_response_chunk", "content": content_piece}
                            elif ev == "on_chat_model_end":
                                data = event.get("data", {})
                                output = data.get("output")
                                try:
                                    tool_calls_check = getattr(output, 'tool_calls', None)
                                except Exception:
                                    tool_calls_check = None
                                try:
                                    content_preview = getattr(output, 'content', None) or ""
                                except Exception:
                                    content_preview = ""
                                # æ•è·çœŸå®tokenç”¨é‡ï¼ˆè‹¥åº•å±‚è¿”å›ï¼‰
                                try:
                                    usage = getattr(output, 'usage_metadata', None)
                                    if not usage:
                                        meta = getattr(output, 'response_metadata', None) or {}
                                        # å…¼å®¹ä¸åŒSDKå­—æ®µ
                                        usage = meta.get('token_usage') or {
                                            k: meta.get(k) for k in ("input_tokens", "output_tokens", "total_tokens") if k in meta
                                        }
                                    if usage:
                                        # è§„èŒƒåŒ–ä¸ºdict
                                        if not isinstance(usage, dict):
                                            try:
                                                usage = dict(usage)
                                            except Exception:
                                                usage = {"raw": str(usage)}
                                        last_usage = {
                                            "input_tokens": usage.get("input_tokens"),
                                            "output_tokens": usage.get("output_tokens"),
                                            "total_tokens": usage.get("total_tokens") or (
                                                (usage.get("input_tokens") or 0) + (usage.get("output_tokens") or 0)
                                            )
                                        }
                                except Exception:
                                    last_usage = last_usage
                    finally:
                        mcp_logger.setLevel(original_level)
                except Exception as e:
                    error_msg = str(e)
                    print(f"âš ï¸ å·¥å…·åˆ¤å®š(æµå¼)å¤±è´¥ï¼š{error_msg}")
                    
                    # æ£€æŸ¥æ˜¯å¦ä¸ºå¤šæ¨¡æ€æ ¼å¼é”™è¯¯ï¼Œå¦‚æœæ˜¯åˆ™å°è¯•é™çº§é‡è¯•
                    if not multimodal_fallback_attempted and self._is_multimodal_error(error_msg):
                        print(f"ğŸ”„ æ£€æµ‹åˆ°å¤šæ¨¡æ€æ ¼å¼é”™è¯¯ï¼Œå°è¯•é™çº§ä¸ºçº¯æ–‡æœ¬æ¨¡å¼...")
                        
                        # æ ‡è®°è¯¥æ¨¡å‹ä¸æ”¯æŒå¤šæ¨¡æ€
                        self._non_multimodal_models.add(current_model_key)
                        
                        # å‘é€é™çº§æç¤º
                        if not combined_response_started:
                            yield {"type": "ai_response_start", "content": "AIæ­£åœ¨å›å¤..."}
                            combined_response_started = True
                        yield {"type": "ai_response_chunk", "content": "âš ï¸ å½“å‰æ¨¡å‹ä¸æ”¯æŒå›¾ç‰‡è¯†åˆ«ï¼Œå·²è‡ªåŠ¨è½¬æ¢ä¸ºçº¯æ–‡æœ¬æ¨¡å¼å¤„ç†ã€‚\n\n"}
                        
                        # è½¬æ¢æ¶ˆæ¯ä¸ºçº¯æ–‡æœ¬æ ¼å¼å¹¶é‡è¯•
                        text_only_messages = self._convert_multimodal_to_text(tools_messages)
                        multimodal_fallback_attempted = True
                        
                        try:
                            # é™çº§é‡è¯•æ—¶ä¹ŸæŠ‘åˆ¶MCPé”™è¯¯æ—¥å¿—
                            mcp_logger.setLevel(logging.CRITICAL)
                            try:
                                async for event in current_llm_tools.astream_events(text_only_messages, version="v1"):
                                    ev = event.get("event")
                                    if ev == "on_chat_model_stream":
                                        data = event.get("data", {})
                                        chunk = data.get("chunk")
                                        if chunk is None:
                                            continue
                                        try:
                                            content_piece = getattr(chunk, 'content', None)
                                        except Exception:
                                            content_piece = None
                                        if content_piece:
                                            response_started = True
                                            buffered_chunks.append(content_piece)
                                            try:
                                                print(f"ğŸ“¤ [é™çº§LLMæµ] {content_piece}")
                                            except Exception:
                                                pass
                                            yield {"type": "ai_response_chunk", "content": content_piece}
                                    elif ev == "on_chat_model_end":
                                        data = event.get("data", {})
                                        output = data.get("output")
                                        try:
                                            tool_calls_check = getattr(output, 'tool_calls', None)
                                        except Exception:
                                            tool_calls_check = None
                                        try:
                                            content_preview = getattr(output, 'content', None) or ""
                                        except Exception:
                                            content_preview = ""
                            finally:
                                mcp_logger.setLevel(original_level)
                        except Exception as fallback_e:
                            print(f"âŒ é™çº§é‡è¯•ä¹Ÿå¤±è´¥ï¼š{fallback_e}")
                            tool_calls_check = None
                            content_preview = ""
                    else:
                        tool_calls_check = None
                        content_preview = ""

                if tool_calls_check:
                    # åˆå¹¶æ¨¡å¼ï¼šä¸ç»“æŸæ¶ˆæ¯ï¼Œæ’å…¥åˆ†éš”åç»§ç»­æ‰§è¡Œå·¥å…·ï¼Œæœ€ç»ˆä¸€å¹¶ç»“æŸ
                    if response_started and buffered_chunks:
                        yield {"type": "ai_response_chunk", "content": "\n\n"}
                        buffered_chunks = []

                    tool_calls_to_run = tool_calls_check
                    yield {"type": "tool_plan", "content": f"AIå†³å®šè°ƒç”¨ {len(tool_calls_to_run)} ä¸ªå·¥å…·", "tool_count": len(tool_calls_to_run)}
                    # å†™å›assistantå¸¦tool_calls
                    try:
                        shared_history.append({
                            "role": "assistant",
                            "content": "",
                            "tool_calls": tool_calls_to_run
                        })
                    except Exception:
                        shared_history.append({"role": "assistant", "content": ""})

                    # æ‰§è¡Œå·¥å…·ï¼ˆéæµå¼ï¼‰
                    exit_to_stream = False
                    for i, tool_call in enumerate(tool_calls_to_run, 1):
                        if isinstance(tool_call, dict):
                            tool_id = tool_call.get('id') or f"call_{i}"
                            fn = tool_call.get('function') or {}
                            tool_name = fn.get('name') or tool_call.get('name') or ''
                            tool_args_raw = fn.get('arguments') or tool_call.get('args') or {}
                        else:
                            tool_id = getattr(tool_call, 'id', None) or f"call_{i}"
                            tool_name = getattr(tool_call, 'name', '') or ''
                            tool_args_raw = getattr(tool_call, 'args', {}) or {}

                        # è§£æå‚æ•°
                        if isinstance(tool_args_raw, str):
                            try:
                                parsed_args = json.loads(tool_args_raw) if tool_args_raw else {}
                            except Exception:
                                parsed_args = {"$raw": tool_args_raw}
                        elif isinstance(tool_args_raw, dict):
                            parsed_args = tool_args_raw
                        else:
                            parsed_args = {"$raw": str(tool_args_raw)}

                        yield {"type": "tool_start", "tool_id": tool_id, "tool_name": tool_name, "tool_args": parsed_args, "progress": f"{i}/{len(tool_calls_to_run)}"}

                        try:
                            target_tool = None
                            for tool in self.tools:
                                if tool.name == tool_name:
                                    target_tool = tool
                                    break
                            if target_tool is None:
                                error_msg = f"å·¥å…· '{tool_name}' æœªæ‰¾åˆ°"
                                print(f"âŒ {error_msg}")
                                yield {"type": "tool_error", "tool_id": tool_id, "error": error_msg}
                                tool_result = f"é”™è¯¯: {error_msg}"
                            else:
                                # æŠ‘åˆ¶MCPå®¢æˆ·ç«¯åœ¨å·¥å…·è°ƒç”¨æ—¶çš„SSEè§£æé”™è¯¯æ—¥å¿—
                                import logging
                                mcp_logger = logging.getLogger('mcp')
                                original_level = mcp_logger.level
                                mcp_logger.setLevel(logging.CRITICAL)
                                
                                try:
                                    tool_result = await target_tool.ainvoke(parsed_args)
                                    yield {"type": "tool_end", "tool_id": tool_id, "tool_name": tool_name, "result": str(tool_result)}
                                    # ä¸å†æ”¯æŒé€€å‡ºå·¥å…·æ¨¡å¼
                                finally:
                                    mcp_logger.setLevel(original_level)
                        except Exception as e:
                            error_msg = f"å·¥å…·æ‰§è¡Œå‡ºé”™: {e}"
                            print(f"âŒ {error_msg}")
                            yield {"type": "tool_error", "tool_id": tool_id, "error": error_msg}
                            tool_result = f"é”™è¯¯: {error_msg}"

                        # å§‹ç»ˆè¿½åŠ  tool æ¶ˆæ¯ï¼Œæ»¡è¶³ OpenAI å‡½æ•°è°ƒç”¨åè®®è¦æ±‚
                        # å¯¹äºé€€å‡ºå·¥å…·æ¨¡å¼ï¼Œå†…å®¹ä¸ºç®€å•çŠ¶æ€ï¼Œä¸å½±å“åç»­å›ç­”è´¨é‡
                        shared_history.append({
                            "role": "tool",
                            "tool_call_id": tool_id,
                            "name": tool_name,
                            "content": str(tool_result)
                        })

                        if exit_to_stream:
                            break

                    if exit_to_stream:
                        # ä¸å†æ”¯æŒæå‰å¼ºåˆ¶åˆ‡æµå¼ï¼ŒæŒ‰åŸé€»è¾‘ç»§ç»­ä¸‹ä¸€è½®
                        pass
                    else:
                        # å·¥å…·åç»§ç»­ä¸‹ä¸€è½®
                        continue

                # 3) æ— å·¥å…·ï¼šåˆå¹¶æ¨¡å¼
                # è‹¥å…ˆå‰å·²ç»æµå¼è¾“å‡ºè¿‡ç‰‡æ®µï¼Œåˆ™æ­¤å¤„ä¸å†æŠŠæ‰€æœ‰ç‰‡æ®µå†å‘ä¸€æ¬¡ï¼Œåªå‘é€ç»“æŸæ ‡è®°ï¼›
                # è‹¥æ­¤å‰å°šæœªå¼€å§‹ï¼ˆæ— æµå¼ç‰‡æ®µï¼‰ï¼Œåˆ™ä¸€æ¬¡æ€§å‘é€æœ€ç»ˆæ–‡æœ¬å†ç»“æŸã€‚
                final_text = "".join(buffered_chunks) if buffered_chunks else (content_preview or "")
                if combined_response_started:
                    # å·²ç»å¼€å§‹è¿‡ï¼Œé¿å…é‡å¤å†…å®¹
                    yield {"type": "ai_response_end", "content": ""}
                else:
                    yield {"type": "ai_response_start", "content": "AIæ­£åœ¨å›å¤..."}
                    combined_response_started = True
                    if final_text:
                        try:
                            print(f"ğŸ“¤ [æœ€ç»ˆå›å¤æµ] {final_text}")
                        except Exception:
                            pass
                        yield {"type": "ai_response_chunk", "content": final_text}
                    yield {"type": "ai_response_end", "content": ""}
                # åœ¨ç»“æŸåè¡¥å‘tokenç”¨é‡ï¼ˆè‹¥å¯ç”¨ï¼‰
                if last_usage:
                    try:
                        yield {"type": "token_usage", **last_usage}
                    except Exception:
                        pass
                return

            # è½®æ¬¡è€—å°½ï¼šç›´æ¥è¿”å›æç¤ºä¿¡æ¯
            print(f"âš ï¸ è¾¾åˆ°æœ€å¤§æ¨ç†è½®æ•°({max_rounds})ï¼Œç›´æ¥è¿”å›æç¤ºä¿¡æ¯")
            final_text = "å·²è¾¾åˆ°æœ€å¤§æ¨ç†è½®æ•°ï¼Œè¯·ç¼©å°é—®é¢˜èŒƒå›´æˆ–ç¨åé‡è¯•ã€‚"
            yield {"type": "ai_response_start", "content": "AIæ­£åœ¨å›å¤..."}
            yield {"type": "ai_response_chunk", "content": final_text}
            yield {"type": "ai_response_end", "content": final_text}
            return
        except Exception as e:
            import traceback
            print(f"âŒ chat_stream å¼‚å¸¸: {e}")
            print("ğŸ“‹ è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            traceback.print_exc()
            yield {"type": "error", "content": f"å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {str(e)}"}

    def get_tools_info(self) -> Dict[str, Any]:
        """è·å–å·¥å…·ä¿¡æ¯åˆ—è¡¨ï¼ŒæŒ‰MCPæœåŠ¡å™¨åˆ†ç»„"""
        return self.tools_manager.get_tools_info()

    async def close(self):
        """å…³é—­è¿æ¥"""
        try:
            await self.tools_manager.close()
        except:
            pass
